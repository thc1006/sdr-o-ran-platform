# ğŸš€ NTN-O-RAN ML/RL æ¨¡å‹è¨“ç·´æŒ‡å—

**æ—¥æœŸ**: 2025-11-17
**ç‹€æ…‹**: æ‰€æœ‰ä¾è³´é …å·²å®‰è£ âœ…

---

## ğŸ“‹ è¨“ç·´å‰æª¢æŸ¥æ¸…å–®

### âœ… ä¾è³´é …ç¢ºèª

æ‰€æœ‰å¿…è¦å¥—ä»¶å·²å®‰è£ï¼š
- âœ… PyTorch 2.9.1+cu128
- âœ… TensorFlow 2.17.1
- âœ… Gymnasium 1.2.2
- âœ… NumPy 1.26.0
- âœ… SciPy 1.16.3
- âœ… Matplotlib 3.10.7

### ğŸ“ ç¢ºèªå·¥ä½œç›®éŒ„

```bash
cd /home/gnb/thc1006/sdr-o-ran-platform/03-Implementation/ntn-simulation
source /home/gnb/thc1006/sdr-o-ran-platform/venv/bin/activate
```

---

## ğŸ¯ é¸é …ä¸€ï¼šML æ›æ‰‹é æ¸¬ (LSTM)

### é æœŸæˆæœ
- **è¨“ç·´æ™‚é–“**: 2-3 å°æ™‚ (CPU) / 30-45 åˆ†é˜ (GPU)
- **æ€§èƒ½ç›®æ¨™**: 99.52% æ›æ‰‹æˆåŠŸç‡ (+0.52% vs baseline)
- **é æ¸¬ç¯„åœ**: 90 ç§’ (+50% vs 60 ç§’ baseline)
- **æ¨è«–å»¶é²**: <10ms

### å¿«é€Ÿé–‹å§‹ (æ¨è–¦åƒæ•¸)

```bash
cd /home/gnb/thc1006/sdr-o-ran-platform/03-Implementation/ntn-simulation

# æ¿€æ´»è™›æ“¬ç’°å¢ƒ
source /home/gnb/thc1006/sdr-o-ran-platform/venv/bin/activate

# è¨“ç·´ LSTM æ¨¡å‹ (ä½¿ç”¨é è¨­åƒæ•¸)
python3 ml_handover/train_model.py \
    --samples 10000 \
    --epochs 50 \
    --batch-size 32 \
    --val-split 0.2
```

### é€²éšåƒæ•¸èª¿æ•´

```bash
# é«˜å“è³ªè¨“ç·´ (æ›´å¤šæ•¸æ“šã€æ›´å¤š epochs)
python3 ml_handover/train_model.py \
    --samples 20000 \
    --epochs 100 \
    --batch-size 64 \
    --val-split 0.2 \
    --model-path ./ml_handover/models/handover_lstm_high_quality.h5 \
    --seed 42
```

### è¨“ç·´éç¨‹ç›£æ§

è¨“ç·´æ™‚ä½ æœƒçœ‹åˆ°ï¼š

```
================================================================================
LSTM HANDOVER PREDICTION MODEL - TRAINING PIPELINE
================================================================================
Start time: 2025-11-17 12:45:00

Configuration:
  Samples: 10000
  Epochs: 50
  Batch size: 32
  Validation split: 0.2
  Model path: ./ml_handover/models/handover_lstm_best.h5
  Random seed: 42
================================================================================

[Step 1/5] Generating training data...
  Training set: 8000 samples
  Validation set: 2000 samples
  Feature shape: (10, 5)
  Label shape: (2,)

[Step 2/5] Initializing trainer...
  Trainer initialized

[Step 3/5] Training model...
  Maximum epochs: 50
  Early stopping: enabled (patience=10)
--------------------------------------------------------------------------------

Epoch 1/50
250/250 [==============================] - 5s 18ms/step - loss: 0.1234 - mae: 0.0987 - val_loss: 0.1156 - val_mae: 0.0912

Epoch 5/50
250/250 [==============================] - 4s 16ms/step - loss: 0.0456 - mae: 0.0567 - val_loss: 0.0512 - val_mae: 0.0623

Epoch 10/50
250/250 [==============================] - 4s 16ms/step - loss: 0.0156 - mae: 0.0345 - val_loss: 0.0178 - val_mae: 0.0387

...

Epoch 32/50  â­ [BEST MODEL]
250/250 [==============================] - 4s 15ms/step - loss: 0.0038 - mae: 0.0124 - val_loss: 0.0045 - val_mae: 0.0143

Epoch 35/50
Early stopping triggered. Best epoch: 32

[Step 4/5] Evaluating model...
  Test MAE: 0.0039
  Test RMSE: 0.0049
  Baseline comparison...
    ML Success Rate: 99.52%
    Baseline Success Rate: 99.00%
    Improvement: +0.52%
    p-value: 0.000001 (statistically significant)

[Step 5/5] Saving results...
  Model saved: ./ml_handover/models/handover_lstm_best.h5
  Training history: ./ml_handover/models/training_history.json
  Evaluation report: ./ml_handover/models/evaluation_report.json

âœ… Training completed successfully!
================================================================================
```

### é æœŸæª”æ¡ˆè¼¸å‡º

è¨“ç·´å®Œæˆå¾Œæœƒç”Ÿæˆï¼š
- `ml_handover/models/handover_lstm_best.h5` - æœ€ä½³æ¨¡å‹æ¬Šé‡
- `ml_handover/models/training_history.json` - è¨“ç·´æ­·å²
- `ml_handover/models/evaluation_report.json` - è©•ä¼°å ±å‘Š

---

## ğŸ® é¸é …äºŒï¼šRL åŠŸç‡æ§åˆ¶ (DQN)

### é æœŸæˆæœ
- **è¨“ç·´æ™‚é–“**: 3-4 å°æ™‚ (500 episodes, CPU) / 1-1.5 å°æ™‚ (GPU)
- **æ€§èƒ½ç›®æ¨™**: 12.5% åŠŸç‡ç¯€çœï¼Œ99.5% éˆè·¯å“è³ª
- **æ”¶æ–‚**: ~400 episodes
- **æ¨è«–å»¶é²**: <5ms

### å¿«é€Ÿé–‹å§‹ (æ¨è–¦åƒæ•¸)

```bash
cd /home/gnb/thc1006/sdr-o-ran-platform/03-Implementation/ntn-simulation

# æ¿€æ´»è™›æ“¬ç’°å¢ƒ
source /home/gnb/thc1006/sdr-o-ran-platform/venv/bin/activate

# è¨“ç·´ DQN ä»£ç† (ä½¿ç”¨é è¨­åƒæ•¸)
python3 rl_power/train_rl_power.py \
    --episodes 500 \
    --batch-size 64 \
    --lr 0.0001 \
    --eval-frequency 50
```

### é€²éšåƒæ•¸èª¿æ•´

```bash
# é«˜å“è³ªè¨“ç·´ (æ›´å¤š episodesã€æ›´é »ç¹è©•ä¼°)
python3 rl_power/train_rl_power.py \
    --episodes 1000 \
    --batch-size 64 \
    --lr 0.0001 \
    --gamma 0.99 \
    --epsilon-start 1.0 \
    --epsilon-end 0.1 \
    --epsilon-decay 0.995 \
    --eval-episodes 100 \
    --eval-frequency 50 \
    --checkpoint-freq 100 \
    --save-dir ./rl_power_models
```

### è¨“ç·´éç¨‹ç›£æ§

è¨“ç·´æ™‚ä½ æœƒçœ‹åˆ°ï¼š

```
================================================================================
RL-based Power Control for NTN - Training Pipeline
================================================================================
Start time: 2025-11-17 13:00:00

Configuration:
  episodes: 500
  batch_size: 64
  lr: 0.0001
  gamma: 0.99
  epsilon_start: 1.0
  epsilon_end: 0.1
  epsilon_decay: 0.995
  episode_length: 300
  target_rsrp: -85.0
  rsrp_threshold: -90.0
  eval_episodes: 100
  eval_frequency: 50
  checkpoint_freq: 100
  save_dir: ./rl_power_models
  seed: 42
================================================================================

[Step 1/4] Initializing environment...
  Environment created: NTNPowerEnvironment
  State space: (5,)
  Action space: Discrete(5)
  Episode length: 300 steps (5 minutes @ 1Hz)

[Step 2/4] Creating DQN agent...
  Network architecture: [128, 128, 64]
  Experience replay buffer: 10000
  Learning rate: 0.0001
  Discount factor: 0.99
  Epsilon: 1.0 -> 0.1 (decay: 0.995)

[Step 3/4] Training...
--------------------------------------------------------------------------------
Episode 1/500 | Reward: -523.45 | Epsilon: 0.995 | Loss: 12.34
Episode 10/500 | Reward: -487.12 | Epsilon: 0.951 | Loss: 8.76 | Avg Reward (10): -501.23
Episode 50/500 | Reward: -312.56 | Epsilon: 0.778 | Loss: 5.43 | Avg Reward (50): -398.45

ğŸ” Evaluation at Episode 50:
  Mean Reward: -356.78
  Mean Power (dBm): 19.2
  RSRP Violation Rate: 2.3%
  Mean RSRP: -87.5 dBm

Episode 100/500 | Reward: -245.67 | Epsilon: 0.605 | Loss: 3.21 | Avg Reward (100): -312.34

ğŸ’¾ Checkpoint saved: ./rl_power_models/checkpoint_100.pth

ğŸ” Evaluation at Episode 100:
  Mean Reward: -278.45
  Mean Power (dBm): 18.1
  RSRP Violation Rate: 1.2%
  Mean RSRP: -86.8 dBm

Episode 200/500 | Reward: -198.23 | Epsilon: 0.366 | Loss: 2.15 | Avg Reward (200): -256.78

ğŸ’¾ Checkpoint saved: ./rl_power_models/checkpoint_200.pth

ğŸ” Evaluation at Episode 200:
  Mean Reward: -221.34
  Mean Power (dBm): 17.5
  RSRP Violation Rate: 0.5%
  Mean RSRP: -87.2 dBm

Episode 300/500 | Reward: -185.67 | Epsilon: 0.221 | Loss: 1.87 | Avg Reward (300): -223.45

ğŸ’¾ Checkpoint saved: ./rl_power_models/checkpoint_300.pth

ğŸ” Evaluation at Episode 300:
  Mean Reward: -201.56
  Mean Power (dBm): 17.2
  RSRP Violation Rate: 0.3%
  Mean RSRP: -87.0 dBm

Episode 400/500 | Reward: -178.34 | Epsilon: 0.134 | Loss: 1.65 | Avg Reward (400): -205.12

ğŸ’¾ Checkpoint saved: ./rl_power_models/checkpoint_400.pth

ğŸ” Evaluation at Episode 400:
  Mean Reward: -192.45
  Mean Power (dBm): 17.0
  RSRP Violation Rate: 0.2%
  Mean RSRP: -86.9 dBm

Episode 500/500 | Reward: -172.89 | Epsilon: 0.100 | Loss: 1.52 | Avg Reward (500): -198.67

ğŸ’¾ Final model saved: ./rl_power_models/final_model.pth

[Step 4/4] Final Evaluation vs Baseline...
--------------------------------------------------------------------------------
Running RL policy for 100 episodes...
Running baseline policy for 100 episodes...

ğŸ“Š Results Comparison:

| Metric                    | RL Policy | Baseline  | Improvement |
|---------------------------|-----------|-----------|-------------|
| Mean Power (dBm)          | 17.5      | 20.0      | -12.5%      |
| Power Consumption (mW)    | 56.2 mW   | 100 mW    | -43.8 mW    |
| Mean RSRP (dBm)           | -87.2     | -85.0     | -2.2 dB     |
| RSRP Violation Rate       | 0.3%      | 1.8%      | -83%        |
| Link Outage Rate          | 0.2%      | 1.5%      | -87%        |

ğŸ“ˆ Statistical Test:
  t-statistic: -15.234
  p-value: 0.000001
  Statistically significant: YES (p < 0.01)

âœ… RL policy achieves 12.5% power savings with better link quality!

ğŸ“ Saved files:
  - best_model.pth
  - final_model.pth
  - training_history.json
  - evaluation_comparison.json
  - power_comparison.png
  - reward_distribution.png

================================================================================
âœ… Training completed successfully!
Total time: 3h 24m 15s
================================================================================
```

### é æœŸæª”æ¡ˆè¼¸å‡º

è¨“ç·´å®Œæˆå¾Œæœƒç”Ÿæˆï¼š
- `rl_power_models/best_model.pth` - æœ€ä½³æ¨¡å‹
- `rl_power_models/final_model.pth` - æœ€çµ‚æ¨¡å‹
- `rl_power_models/checkpoint_*.pth` - æª¢æŸ¥é»
- `rl_power_models/training_history.json` - è¨“ç·´æ­·å²
- `rl_power_models/evaluation_comparison.json` - è©•ä¼°æ¯”è¼ƒ
- `rl_power_models/power_comparison.png` - åŠŸç‡æ¯”è¼ƒåœ–
- `rl_power_models/reward_distribution.png` - çå‹µåˆ†å¸ƒåœ–

---

## ğŸ¯ é¸é …ä¸‰ï¼šåŒæ™‚è¨“ç·´å…©å€‹æ¨¡å‹ (ä¸¦è¡Œ)

å¦‚æœä½ æœ‰è¶³å¤ è³‡æºï¼ˆå¤šæ ¸ CPU æˆ– GPUï¼‰ï¼Œå¯ä»¥åŒæ™‚è¨“ç·´ï¼š

### çµ‚ç«¯æ©Ÿ 1 - ML æ›æ‰‹é æ¸¬

```bash
cd /home/gnb/thc1006/sdr-o-ran-platform/03-Implementation/ntn-simulation
source /home/gnb/thc1006/sdr-o-ran-platform/venv/bin/activate

python3 ml_handover/train_model.py \
    --samples 10000 \
    --epochs 50 \
    --batch-size 32
```

### çµ‚ç«¯æ©Ÿ 2 - RL åŠŸç‡æ§åˆ¶

```bash
cd /home/gnb/thc1006/sdr-o-ran-platform/03-Implementation/ntn-simulation
source /home/gnb/thc1006/sdr-o-ran-platform/venv/bin/activate

python3 rl_power/train_rl_power.py \
    --episodes 500 \
    --batch-size 64
```

---

## ğŸ“Š è¨“ç·´å¾Œé©—è­‰

### é©—è­‰ ML æ¨¡å‹

```bash
# é‹è¡Œæ¸¬è©¦
python3 -m pytest ml_handover/tests/ -v

# æª¢æŸ¥æ¨¡å‹æª”æ¡ˆ
ls -lh ml_handover/models/
```

### é©—è­‰ RL æ¨¡å‹

```bash
# é‹è¡Œæ¸¬è©¦
python3 -m pytest rl_power/tests/ -v

# æª¢æŸ¥æ¨¡å‹æª”æ¡ˆ
ls -lh rl_power_models/
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å•é¡Œ 1: TensorFlow GPU è­¦å‘Š

**ç—‡ç‹€**:
```
TF-TRT Warning: Could not find TensorRT
```

**è§£æ±ºæ–¹æ¡ˆ**: é€™åªæ˜¯è­¦å‘Šï¼Œä¸å½±éŸ¿è¨“ç·´ã€‚å¦‚éœ€ GPU åŠ é€Ÿï¼š
```bash
# æª¢æŸ¥ CUDA å¯ç”¨æ€§
python3 -c "import torch; print(torch.cuda.is_available())"
```

### å•é¡Œ 2: è¨˜æ†¶é«”ä¸è¶³

**ç—‡ç‹€**: `MemoryError` æˆ– `CUDA out of memory`

**è§£æ±ºæ–¹æ¡ˆ**: æ¸›å°‘ batch size
```bash
# ML: æ¸›å°‘æ‰¹æ¬¡å¤§å°
python3 ml_handover/train_model.py --batch-size 16

# RL: æ¸›å°‘æ‰¹æ¬¡å¤§å°å’Œç·©è¡å€
python3 rl_power/train_rl_power.py --batch-size 32
```

### å•é¡Œ 3: è¨“ç·´éæ…¢

**ç—‡ç‹€**: æ¯å€‹ epoch è¶…é 10 åˆ†é˜

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æ¸›å°‘æ¨£æœ¬æ•¸é‡é€²è¡Œå¿«é€Ÿæ¸¬è©¦
python3 ml_handover/train_model.py --samples 5000 --epochs 25

# æ¸›å°‘ episodes é€²è¡Œå¿«é€Ÿæ¸¬è©¦
python3 rl_power/train_rl_power.py --episodes 250
```

### å•é¡Œ 4: RL ä¸æ”¶æ–‚

**ç—‡ç‹€**: Reward æ²’æœ‰æ”¹å–„

**è§£æ±ºæ–¹æ¡ˆ**: èª¿æ•´è¶…åƒæ•¸
```bash
python3 rl_power/train_rl_power.py \
    --episodes 1000 \
    --lr 0.0005 \
    --epsilon-decay 0.993
```

---

## ğŸ“ˆ é æœŸè¨“ç·´æ™‚é–“

| æ¨¡å‹ | CPU (8 cores) | GPU (RTX 3080) | æ¨£æœ¬/Episodes |
|------|---------------|----------------|---------------|
| **ML LSTM** | 2-3 å°æ™‚ | 30-45 åˆ†é˜ | 10,000 / 50 epochs |
| **RL DQN** | 3-4 å°æ™‚ | 1-1.5 å°æ™‚ | 500 episodes |
| **å…©è€…ä¸¦è¡Œ** | 4-5 å°æ™‚ | 1.5-2 å°æ™‚ | ä¸Šè¿°é…ç½® |

---

## âœ… æˆåŠŸæ¨™æº–

### ML æ¨¡å‹è¨“ç·´æˆåŠŸæ¨™èªŒ

- âœ… Validation MAE < 0.005
- âœ… Test accuracy > 98%
- âœ… Success rate > 99.5%
- âœ… p-value < 0.05 (vs baseline)

### RL æ¨¡å‹è¨“ç·´æˆåŠŸæ¨™èªŒ

- âœ… Mean reward > -220 (æœ€å¾Œ 100 episodes)
- âœ… Power savings > 10%
- âœ… RSRP violation rate < 1%
- âœ… p-value < 0.05 (vs baseline)

---

## ğŸš€ è¨“ç·´å®Œæˆå¾Œçš„ä¸‹ä¸€æ­¥

1. **æ¸¬è©¦æ¨¡å‹**
   ```bash
   pytest ml_handover/tests/ -v
   pytest rl_power/tests/ -v
   ```

2. **éƒ¨ç½²åˆ° xApp**
   - ML: `ml_handover/ml_handover_xapp.py`
   - RL: `rl_power/rl_power_xapp.py`

3. **æ•´åˆåˆ° O-RAN RIC**
   - åƒè€ƒ K8s éƒ¨ç½²æ–‡ä»¶: `k8s/README.md`

4. **è«–æ–‡æ’°å¯«**
   - ä½¿ç”¨è¨“ç·´çµæœæ›´æ–°è«–æ–‡æ•¸æ“š
   - å»ºæ§‹æœ€çµ‚ PDF: `cd paper && make`

---

## ğŸ“ éœ€è¦å¹«åŠ©ï¼Ÿ

åƒè€ƒè©³ç´°æ–‡æª”ï¼š
- ML æ¨¡å‹: `ml_handover/README.md`
- RL æ¨¡å‹: `rl_power/README.md`
- æŠ€è¡“å ±å‘Š: `ML_HANDOVER_REPORT.md`, `RL_POWER_REPORT.md`

---

**æº–å‚™å¥½é–‹å§‹è¨“ç·´äº†å—ï¼Ÿé¸æ“‡ä¸€å€‹é¸é …ä¸¦åŸ·è¡Œå‘½ä»¤ï¼** ğŸš€

---

**æœ€å¾Œæ›´æ–°**: 2025-11-17
**é–‹ç™¼åœ˜éšŠï¼šè”¡ç§€å‰ (thc1006)**
