# RL Power Control - Restructuring Report

**Date**: 2025-11-17
**Status**: âœ… Environment Fixed, ğŸ”„ Retraining in Progress (Episode 480/1500)

## Executive Summary

The RL power control model failed in initial training due to **critical physics errors** in the environment. After comprehensive restructuring of the link budget calculations and reward function, the environment now produces realistic RSRP values and the model is training successfully.

---

## Problem Diagnosis

### Initial Failure Symptoms
- **RSRP**: -144.76 dBm (54.76 dB below -90 dBm threshold) âŒ
- **Violation Rate**: 100% (all timesteps violated RSRP constraint) âŒ
- **Power Savings**: 0% (impossible to learn with constant violations) âŒ
- **p-value**: 1.0 (no statistical significance) âŒ

### Root Cause Analysis

#### 1. Incorrect Transmit Power (`ntn_env.py:61-63`)
```python
# BEFORE (WRONG):
self.initial_power_dbm = 20.0  # Too low for LEO satellite
self.max_power_dbm = 23.0
self.min_power_dbm = 0.0

# AFTER (CORRECT):
self.initial_power_dbm = 46.0  # Realistic LEO satellite power
self.max_power_dbm = 49.0
self.min_power_dbm = 26.0
```

**Impact**: +26 dB power increase (20â†’46 dBm)

#### 2. Insufficient Antenna Gain (`ntn_env.py:325-330`)
```python
# BEFORE (WRONG):
antenna_gain_db = 10.0 + 5.0 * np.sin(np.radians(elevation_deg))
# Result: 10-15 dB (too low)

# AFTER (CORRECT):
base_antenna_gain = 45.0  # Combined Tx + Rx antennas
elevation_factor = 5.0 * np.sin(np.radians(elevation_deg))
antenna_gain_db = base_antenna_gain + elevation_factor
# Result: 45-50 dB (realistic)
```

**Impact**: +35 dB antenna gain increase

#### 3. Link Budget Verification

**Before Fix**:
```
Tx Power:       20.0 dBm
FSPL:         -156.0 dB  (800 km, 2 GHz)
Antenna Gain:  +12.5 dB
Atmospheric:    -0.5 dB
Rain Atten:   -0 to -50 dB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RSRP:       -124 to -174 dBm âŒ (way below -90 dBm threshold)
```

**After Fix**:
```
Tx Power:       46.0 dBm  (+26 dB)
FSPL:         -156.0 dB  (unchanged)
Antenna Gain:  +48.0 dB  (+35 dB)
Atmospheric:    -0.5 dB  (unchanged)
Rain Atten:   -0 to -50 dB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RSRP:       -62 to -112 dBm âœ… (mostly above -90 dBm threshold!)
```

**Net Improvement**: +61 dB in received signal strength

#### 4. Sparse Reward Function (`ntn_env.py:456-483`)

**Before**: Binary reward (penalty if RSRP < threshold, small power penalty otherwise)

**After**: Shaped reward with:
- Exponential RSRP violation penalty (severity-weighted)
- Normalized power consumption penalty
- Efficiency bonus for operating near target RSRP (-85 dBm)
- Penalty for excessive margin (wasting power)

---

## Fixes Implemented

### 1. Environment Physics (`rl_power/ntn_env.py`)

| Parameter | Before | After | Change |
|-----------|--------|-------|--------|
| Initial Power | 20.0 dBm | 46.0 dBm | +26 dB |
| Max Power | 23.0 dBm | 49.0 dBm | +26 dB |
| Min Power | 0.0 dBm | 26.0 dBm | +26 dB |
| Antenna Gain | 10-15 dB | 45-50 dB | +35 dB |
| **RSRP Range** | **-124 to -174 dBm** | **-62 to -112 dBm** | **+61 dB** |

### 2. Reward Function Redesign (`rl_power/ntn_env.py:459-504`)

**New Reward Structure**:
```python
if RSRP < threshold:
    # Exponential penalty for violations
    violation_db = threshold - RSRP
    penalty = 100 * (1 + violation_dbÂ² / 100)
    return -penalty
else:
    # Optimize for power efficiency
    power_penalty = 10 * (power - min_power) / (max_power - min_power)

    # Efficiency bonus for near-target operation
    if |RSRP - target| < 2 dB:
        efficiency_bonus = 5.0  # Excellent
    elif |RSRP - target| < 5 dB:
        efficiency_bonus = 2.0  # Good
    else:
        efficiency_bonus = 0.0

    return -power_penalty + efficiency_bonus
```

### 3. Training Configuration Fixes (`rl_power/train_rl_power.py:109-119`)

**Removed hardcoded power overrides** that were nullifying environment defaults:
```python
# REMOVED these lines:
'initial_power_dbm': 20.0,  # Was overriding fixed environment
'max_power_dbm': 23.0,
'min_power_dbm': 0.0,
```

---

## Retraining Configuration

### Optimized Hyperparameters
```
Episodes:        1500  (3Ã— increase for convergence)
Batch Size:      128   (2Ã— increase for faster learning)
Learning Rate:   0.0003 (3Ã— increase from 0.0001)
Epsilon Start:   1.0
Epsilon End:     0.05
Epsilon Decay:   0.997 (slower for better exploration)
Eval Frequency:  100 episodes
Eval Episodes:   50 (5Ã— increase for robust statistics)
```

### Training Progress (Current: Episode 480/1500)

| Episode | Eval Reward | Improvement | Status |
|---------|-------------|-------------|--------|
| 100 | -1130.30 | baseline | Early exploration |
| 200 | -786.89 | +30.4% | Learning! |
| **300** | **-468.66** | **+58.5%** | **âœ… BEST MODEL** |
| 400 | -580.65 | +48.6% | Slight regression (normal) |
| 500 | TBD | TBD | Training... |
| 1000 | TBD | TBD | Training... |
| 1500 | TBD | TBD | Final result |

### Best Model Checkpoint
- **Episode**: 300
- **Mean Reward**: -468.66 Â± 88.26
- **Min/Max**: -629.88 / -302.90
- **Episode Length**: 15.4 steps (avg)
- **Saved**: `rl_power_models/best_model.pth`

---

## Verification Results

### Environment Test (Post-Fix)
```
Initial State:
  Tx Power: 46.0 dBm  âœ…
  Elevation: 38.7Â°
  Slant Range: 5797.8 km
  RSRP: -80.3 dBm  âœ… (9.7 dB above threshold!)
  RSRP Threshold: -90.0 dBm
  RSRP Target: -85.0 dBm

10-Step Test:
  7/10 steps maintained RSRP > -90 dBm  âœ…
  3/10 violations when power reduced too aggressively
  Rewards: -4.6 to -6.3 (acceptable range)
```

---

## Expected Final Results

### Performance Targets
- **Power Savings**: 10-15% (vs rule-based baseline)
- **RSRP Violation Rate**: <1% (vs previous 100%)
- **Statistical Significance**: p < 0.05
- **Mean RSRP**: -85 to -90 dBm (optimal range)

### Success Criteria
âœ… **Primary**: RSRP violations < 1%
âœ… **Secondary**: Any measurable power savings with p < 0.05
ğŸ¯ **Stretch**: 15%+ power savings with p < 0.01

---

## Timeline

- **12:50 UTC+8**: Initial training failed (100% RSRP violations)
- **13:30 UTC+8**: Root cause identified (link budget errors)
- **14:00 UTC+8**: Environment physics fixed (+61 dB RSRP improvement)
- **14:15 UTC+8**: Reward function redesigned (shaped rewards)
- **14:30 UTC+8**: Retraining started (1500 episodes)
- **~17:00 UTC+8**: Expected completion (ETA: 2.5-3 hours)

---

## Technical Details

### Link Budget Formula (Fixed)
```python
def _calculate_rsrp(tx_power, slant_range, elevation, rain_rate):
    # Free Space Path Loss (Friis)
    distance_m = slant_range_km * 1000
    wavelength_m = c / freq_hz
    fspl_db = 20*log10(distance_m) + 20*log10(freq_hz) - 147.55

    # Rain attenuation (ITU-R P.618)
    effective_length = 5.0 / sin(elevation)
    specific_atten = k * (rain_rate ** alpha)
    rain_atten_db = specific_atten * effective_length

    # Combined antenna gains (NEW: realistic values)
    satellite_antenna = 25 dBi  # Typical LEO satellite
    ground_terminal = 20 dBi    # Typical ground station
    elevation_factor = 5 * sin(elevation)  # Multipath reduction
    antenna_gain = 45 + elevation_factor   # Total: 45-50 dB

    # Link budget
    rsrp = tx_power - fspl_db - rain_atten_db + antenna_gain - 0.5
    return rsrp
```

### Typical Satellite Communication Parameters
- **LEO Altitude**: 600 km (Starlink: 550 km, OneWeb: 1200 km)
- **Slant Range**: 600-2000 km (elevation dependent)
- **Satellite Tx Power**: 40-50 dBm (typical)
- **Carrier Frequency**: 2 GHz (S-band) or 10-30 GHz (Ka-band)
- **Satellite Antenna**: 20-30 dBi
- **Ground Terminal**: 15-25 dBi (consumer: 12-18 dBi)

---

## Lessons Learned

### 1. Environment Validation is Critical
Always validate RL environments with **physics-based sanity checks** before training:
- Are state values in realistic ranges?
- Does the reward function have sufficient gradient?
- Can the agent actually influence the outcome?

### 2. Link Budget Calculations
For satellite communications:
- Path loss at 800 km, 2 GHz â‰ˆ 156 dB
- Need 40-50 dBm Tx power + 40-50 dB antenna gains to achieve -80 to -90 dBm RSRP
- 20 dBm Tx + 12 dB antenna = -124 dBm RSRP (unworkable)

### 3. Reward Shaping
Sparse rewards (binary penalty) don't work for continuous control:
- Need shaped rewards with intermediate goals
- Exponential penalties for constraint violations
- Efficiency bonuses for near-optimal operation

### 4. Hyperparameter Tuning
For difficult tasks:
- Increase episodes (500 â†’ 1500)
- Increase batch size (64 â†’ 128)
- Increase learning rate (0.0001 â†’ 0.0003)
- Increase evaluation samples (10 â†’ 50)

---

## Next Steps

1. **Monitor Training** (Current: Episode 480/1500)
   - Check Episode 1000 results
   - Monitor for convergence or divergence
   - Save checkpoints at 250, 500, 750, 1000, 1250, 1500

2. **Final Evaluation** (After Episode 1500)
   - Compare with rule-based baseline
   - Statistical testing (t-test, p-value)
   - Generate performance plots
   - Calculate power savings %

3. **Documentation Update**
   - Update TRAINING-RESULTS-REPORT.md
   - Update FINAL-STATUS.txt
   - Update IEEE paper with RL results (if successful)

4. **Model Deployment** (If >10% savings, p<0.05)
   - Save final model to production path
   - Create inference script for xApp
   - Update K8s deployment manifests

---

## Conclusion

The RL power control environment has been **successfully restructured** with:
- âœ… **+61 dB RSRP improvement** (physics fixes)
- âœ… **Shaped reward function** (learning signal)
- âœ… **58.5% reward improvement** (Episode 300 vs 100)
- ğŸ”„ **Training in progress** (Episode 480/1500)

**Expected ETA for full results**: ~2-3 hours (by 17:00 UTC+8)

The environment now produces realistic satellite link budget calculations and the agent is learning effectively. Early results (Episode 300: -468.66 reward) show **significant improvement** over initial random policy (-1130.30 reward).

---

**Report Generated**: 2025-11-17 14:45 UTC+8
**Training Monitor**: Background Task 5593cc
**Log File**: `rl_power_training_v2.log`
